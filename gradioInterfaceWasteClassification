import os
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import models, transforms, datasets
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
import gradio as gr
import seaborn as sns
import matplotlib.pyplot as plt
import tempfile
from PIL import Image
from sklearn.model_selection import train_test_split
import shutil

# ---- –ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è ----
CATEGORIES = ['organic', 'recyclable']
ORIG_DIR = '/content/waste_clasification_image_sets'
SPLIT_DIR = '/content/waste_clasification_split'
CHECKPOINT_PATH = '/content/best_vit_waste_classifier.pth'
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


# ---- –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö ----
def prepare_data():
    if os.path.exists(SPLIT_DIR):
        shutil.rmtree(SPLIT_DIR)
    for split in ['train', 'val']:
        for cat in CATEGORIES:
            os.makedirs(os.path.join(SPLIT_DIR, split, cat), exist_ok=True)
    for cat in CATEGORIES:
        src = os.path.join(ORIG_DIR, cat)
        imgs = [f for f in os.listdir(src) if os.path.isfile(os.path.join(src, f))]
        train_imgs, val_imgs = train_test_split(imgs, test_size=0.2, random_state=42)
        for img in train_imgs:
            shutil.copyfile(os.path.join(src, img), os.path.join(SPLIT_DIR, 'train', cat, img))
        for img in val_imgs:
            shutil.copyfile(os.path.join(src, img), os.path.join(SPLIT_DIR, 'val', cat, img))


# ---- –ü–æ–±—É–¥–æ–≤–∞ –º–æ–¥–µ–ª—ñ ----
def build_model():
    model = models.vit_b_16(weights=None)
    model.heads.head = nn.Linear(model.heads.head.in_features, len(CATEGORIES))
    return model.to(device)


# ---- –ì–ª–æ–±–∞–ª—å–Ω–∞ –∑–º—ñ–Ω–Ω–∞ –¥–ª—è –º–æ–¥–µ–ª—ñ –ø—ñ—Å–ª—è –Ω–∞–≤—á–∞–Ω–Ω—è –∞–±–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è ----
model = build_model()
if os.path.exists(CHECKPOINT_PATH):
    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    print("‚úÖ –í–∞–≥–∏ –º–æ–¥–µ–ª—ñ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ!")


# ---- –ù–∞–≤—á–∞–Ω–Ω—è ----
def get_dataloaders(batch_size):
    transform_train = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.5]*3, [0.5]*3)
    ])
    transform_val = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize([0.5]*3, [0.5]*3)
    ])
    train_dataset = datasets.ImageFolder(os.path.join(SPLIT_DIR, 'train'), transform=transform_train)
    val_dataset = datasets.ImageFolder(os.path.join(SPLIT_DIR, 'val'), transform=transform_val)

    return {
        'train': torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True),
        'val': torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    }


def train_model(epochs, batch_size, lr):
    global model
    prepare_data()
    dataloaders = get_dataloaders(batch_size)
    model = build_model()
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    model.train()
    history = ""
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloaders['train']:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        # –í–∞–ª—ñ–¥–∞—Ü—ñ—è
        model.eval()
        all_preds, all_labels = [], []
        with torch.no_grad():
            for inputs, labels in dataloaders['val']:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        acc = accuracy_score(all_labels, all_preds)
        f1 = f1_score(all_labels, all_preds, average='macro')

        history += f"üìò Epoch {epoch+1}/{epochs}: Loss = {running_loss/len(dataloaders['train']):.4f}, Accuracy = {acc:.4f}, F1 = {f1:.4f}\n"
        model.train()

    torch.save({'model_state_dict': model.state_dict()}, CHECKPOINT_PATH)
    model.eval()
    return history


# ---- –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è ----
transform_predict = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.5]*3, [0.5]*3)
])

def classify_image(img: Image.Image):
    try:
        tensor = transform_predict(img).unsqueeze(0).to(device)
        with torch.no_grad():
            output = model(tensor)
            probs = torch.softmax(output, dim=1).cpu().numpy()[0]
            pred_idx = probs.argmax()
            pred_label = CATEGORIES[pred_idx]
        
        # –ü–æ–±—É–¥—É—î–º–æ —Ç–∞–±–ª–∏—á–∫—É –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç–µ–π
        prob_table = "| –ö–∞—Ç–µ–≥–æ—Ä—ñ—è | –ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å |\n|-----------|--------------|\n"
        for i, category in enumerate(CATEGORIES):
            prob_table += f"| {category.title()} | {probs[i]*100:.2f}% |\n"

        result = f"### üßæ –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó: **{pred_label.upper()}**\n\n"
        result += prob_table
        result += "\n---\n"
        result += "üß† _–¶—è –º–æ–¥–µ–ª—å –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î Vision Transformer –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –∑–æ–±—Ä–∞–∂–µ–Ω—å –ø–æ–±—É—Ç–æ–≤–∏—Ö –≤—ñ–¥—Ö–æ–¥—ñ–≤._"

        return result
    except Exception as e:
        return f"‚ùå –ü–æ–º–∏–ª–∫–∞: {e}"

# ---- –ú–∞—Ç—Ä–∏—Ü—è –ø–ª—É—Ç–∞–Ω–∏–Ω–∏ + –ú–µ—Ç—Ä–∏–∫–∏ ----
def show_confusion_matrix():
    dataloaders = get_dataloaders(batch_size=16)
    all_preds, all_labels = [], []

    with torch.no_grad():
        for inputs, labels in dataloaders['val']:
            inputs = inputs.to(device)
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.numpy())

    # --- –û–±—á–∏—Å–ª–µ–Ω–Ω—è –º–∞—Ç—Ä–∏—Ü—ñ –ø–ª—É—Ç–∞–Ω–∏–Ω–∏ ---
    cm = confusion_matrix(all_labels, all_preds)
    fig, ax = plt.subplots(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=CATEGORIES, yticklabels=CATEGORIES, ax=ax)
    ax.set_xlabel("–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–æ")
    ax.set_ylabel("–§–∞–∫—Ç")
    ax.set_title("–ú–∞—Ç—Ä–∏—Ü—è –ø–ª—É—Ç–∞–Ω–∏–Ω–∏")

    # --- –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è ---
    tmp_file = tempfile.NamedTemporaryFile(suffix=".png", delete=False)
    fig.savefig(tmp_file.name)
    plt.close(fig)

    # --- –†–æ–∑—Ä–∞—Ö—É–Ω–æ–∫ –º–µ—Ç—Ä–∏–∫ ---
    acc = accuracy_score(all_labels, all_preds)
    prec = precision_score(all_labels, all_preds, average='macro')
    rec = recall_score(all_labels, all_preds, average='macro')
    f1 = f1_score(all_labels, all_preds, average='macro')

    # --- –§–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è —Ç–µ–∫—Å—Ç—É ---
    metrics_text = (
        f"### üìä –ú–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó\n"
        f"- **Accuracy:** {acc:.4f}\n"
        f"- **Precision (macro):** {prec:.4f}\n"
        f"- **Recall (macro):** {rec:.4f}\n"
        f"- **F1-score (macro):** {f1:.4f}\n"
    )

    return tmp_file.name, metrics_text
import gradio as gr
# ---- Gradio –Ü–Ω—Ç–µ—Ä—Ñ–µ–π—Å ----
theme = gr.themes.Default(primary_hue="blue", secondary_hue="green")

train_interface = gr.Interface(
    fn=train_model,
    inputs=[
        gr.Slider(1, 10, value=3, step=1, label="üìÖ –ö—ñ–ª—å–∫—ñ—Å—Ç—å –µ–ø–æ—Ö"),
        gr.Slider(8, 64, value=16, step=8, label="üì¶ Batch size"),
        gr.Slider(1e-6, 1e-3, value=3e-5, step=1e-6, label="‚öôÔ∏è Learning rate")
    ],
    outputs=gr.Text(label="üìà –•—ñ–¥ –Ω–∞–≤—á–∞–Ω–Ω—è"),
    title="üß† –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ Vision Transformer",
    description=(
        "–ù–∞–≤—á—ñ—Ç—å –º–æ–¥–µ–ª—å –∫–ª–∞—Å–∏—Ñ—ñ–∫—É–≤–∞—Ç–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –ø–æ–±—É—Ç–æ–≤–∏—Ö –≤—ñ–¥—Ö–æ–¥—ñ–≤.\n\n"
        "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:**\n"
        "- 3-5 –µ–ø–æ—Ö –¥–ª—è —à–≤–∏–¥–∫–æ—ó –ø–µ—Ä–µ–≤—ñ—Ä–∫–∏\n"
        "- Learning rate ‚âà 3e-5"
    )
)

predict_interface = gr.Interface(
    fn=classify_image,
    inputs=gr.Image(type='pil', label="üñºÔ∏è –ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è"),
    outputs=gr.Markdown(label="üìã –†–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó"),
    title="‚ôªÔ∏è –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –ø–æ–±—É—Ç–æ–≤–∏—Ö –≤—ñ–¥—Ö–æ–¥—ñ–≤",
    description=(
        "–í–∏–∑–Ω–∞—á–∞—î, —á–∏ –∑–æ–±—Ä–∞–∂–µ–Ω–Ω—è –Ω–∞–ª–µ–∂–∏—Ç—å –¥–æ **–æ—Ä–≥–∞–Ω—ñ—á–Ω–∏—Ö** —á–∏ **–ø–µ—Ä–µ—Ä–æ–±–ª—é–≤–∞–Ω–∏—Ö** –≤—ñ–¥—Ö–æ–¥—ñ–≤.\n\n"
        "### ‚ÑπÔ∏è –Ø–∫ –ø—Ä–∞—Ü—é—î –º–æ–¥–µ–ª—å:\n"
        "- –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î Vision Transformer (ViT)\n"
        "- –ü–æ–∫–∞–∑—É—î –π–º–æ–≤—ñ—Ä–Ω–æ—Å—Ç—ñ –¥–ª—è –∫–æ–∂–Ω–æ—ó –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó\n\n"
        "üîî *–ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ —Ñ–æ—Ç–æ —Ç–∞ –æ—Ç—Ä–∏–º–∞–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç*"
    )
)

cm_interface = gr.Interface(
    fn=show_confusion_matrix,
    inputs=[],
    outputs=gr.Image(type="filepath", label="üó∫Ô∏è –ú–∞—Ç—Ä–∏—Ü—è –ø–ª—É—Ç–∞–Ω–∏–Ω–∏"),
    title="üìä –û—Ü—ñ–Ω–∫–∞ —è–∫–æ—Å—Ç—ñ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó",
    description="–ú–∞—Ç—Ä–∏—Ü—è –ø–ª—É—Ç–∞–Ω–∏–Ω–∏ —Ç–∞ –æ—Å–Ω–æ–≤–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ–π–Ω–æ–≥–æ –Ω–∞–±–æ—Ä—É."
)
gr.Markdown("# üè∑Ô∏è –Ü–Ω—Ç–µ–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–æ–≤–∞–Ω–æ–≥–æ —Å–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –≤—ñ–¥—Ö–æ–¥—ñ–≤")

app = gr.TabbedInterface(
    [train_interface, predict_interface, cm_interface],
    ["üß† –ù–∞–≤—á–∞–Ω–Ω—è", "üóëÔ∏è –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è", "üìä –û—Ü—ñ–Ω–∫–∞"],
    theme=theme
)

app.launch()
